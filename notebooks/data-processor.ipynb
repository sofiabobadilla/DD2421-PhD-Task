{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessor\n",
    "\n",
    "Initial experiment setup\n",
    "\n",
    "- Initial idea: Classifier to identify vulnerable lines of code. \n",
    "- Dataset : smartbugs curated- vulnerable lines of code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get the map of contracts and respective line of code\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "dataset= \"smartbugs-curated\"\n",
    "\n",
    "path='../dataset/'+dataset\n",
    "if dataset == 'smartbugs-curated' :\n",
    "    vulnerability_localization= json.load(open(path+'/vulnerabilities.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## now prepare the dataset, for each contract, get the line of code that is vulnerable\n",
    "base_path = f'../dataset/{dataset}'\n",
    "# Function to extract vulnerable lines\n",
    "def extract_vulnerable_lines(contract_data):\n",
    "    vulnerable_lines_data = []\n",
    "    vulnerable_lines = []\n",
    "    \n",
    "    for contract in contract_data:\n",
    "        contract_name = contract['name']\n",
    "        relative_path = contract['path']\n",
    "        full_path = os.path.join(base_path, relative_path)\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(full_path):\n",
    "            print(f\"Warning: File not found - {full_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Read the file content\n",
    "        try:\n",
    "            with open(full_path, 'r', encoding='utf-8') as file:\n",
    "                file_lines = file.readlines()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {full_path}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        # Process vulnerabilities\n",
    "        for vulnerability in contract['vulnerabilities']:\n",
    "            category = vulnerability['category']\n",
    "            \n",
    "            for line_number in vulnerability['lines']:\n",
    "                # Adjust for 0-based indexing\n",
    "                adjusted_line_number = line_number - 1\n",
    "                \n",
    "                # Extract the actual code (if line number is valid)\n",
    "                code_line = \"\"\n",
    "                if 0 <= adjusted_line_number < len(file_lines):\n",
    "                    code_line = file_lines[adjusted_line_number].strip()\n",
    "                else:\n",
    "                    print(f\"Warning: Line {line_number} out of range in {full_path}\")\n",
    "                \n",
    "                vulnerable_lines_data.append({\n",
    "                    'contract_name': contract_name,\n",
    "                    'contract_path': relative_path,\n",
    "                    'full_path': full_path,\n",
    "                    'pragma_version': contract['pragma'],\n",
    "                    'source': contract['source'],\n",
    "                    'line_number': line_number,\n",
    "                    'vulnerability_category': category,\n",
    "                    'code': code_line\n",
    "                })\n",
    "                vulnerable_lines.append(code_line)\n",
    "    \n",
    "    return vulnerable_lines_data, vulnerable_lines\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n",
      "[{'contract_name': 'FibonacciBalance.sol', 'contract_path': 'dataset/access_control/FibonacciBalance.sol', 'full_path': '../dataset/smartbugs-curated/dataset/access_control/FibonacciBalance.sol', 'pragma_version': '0.4.22', 'source': 'https://github.com/sigp/solidity-security-blog', 'line_number': 31, 'vulnerability_category': 'access_control', 'code': 'require(fibonacciLibrary.delegatecall(fibSig, withdrawalCounter));'}, {'contract_name': 'FibonacciBalance.sol', 'contract_path': 'dataset/access_control/FibonacciBalance.sol', 'full_path': '../dataset/smartbugs-curated/dataset/access_control/FibonacciBalance.sol', 'pragma_version': '0.4.22', 'source': 'https://github.com/sigp/solidity-security-blog', 'line_number': 38, 'vulnerability_category': 'access_control', 'code': 'require(fibonacciLibrary.delegatecall(msg.data));'}, {'contract_name': 'arbitrary_location_write_simple.sol', 'contract_path': 'dataset/access_control/arbitrary_location_write_simple.sol', 'full_path': '../dataset/smartbugs-curated/dataset/access_control/arbitrary_location_write_simple.sol', 'pragma_version': '0.4.25', 'source': 'https://smartcontractsecurity.github.io/SWC-registry/docs/SWC-124#arbitrary-location-write-simplesol', 'line_number': 27, 'vulnerability_category': 'access_control', 'code': 'require(0 <= bonusCodes.length); // this condition is always true since array lengths are unsigned'}]\n",
      "['require(fibonacciLibrary.delegatecall(fibSig, withdrawalCounter));', 'require(fibonacciLibrary.delegatecall(msg.data));', 'require(0 <= bonusCodes.length); // this condition is always true since array lengths are unsigned']\n"
     ]
    }
   ],
   "source": [
    "vulnerable_lines_data,vulnerable_lines = extract_vulnerable_lines(vulnerability_localization)\n",
    "print(len(vulnerable_lines_data))\n",
    "print(vulnerable_lines_data[0:3])\n",
    "print(vulnerable_lines[0:3])\n",
    "\n",
    "\n",
    "## storage data\n",
    "with open(f'../dataset/{dataset}_vulnerable_lines.json', 'w') as file:\n",
    "    json.dump(vulnerable_lines_data, file, indent=4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get an equal amount of non-vulnerable lines\n",
    "# current vulnerable lines 222\n",
    "\n",
    "import random\n",
    "import re\n",
    "\n",
    "def get_pragma_version(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        file_lines = file.readlines()\n",
    "    #regex to get the pragma version\n",
    "    while file_lines[0].strip().startswith(\"pragma\") == False:\n",
    "        file_lines.pop(0)\n",
    "    \n",
    "    pragma_line= file_lines[0].strip()\n",
    "\n",
    "    # now only get the pragma version\n",
    "    match = re.search(r'pragma solidity \\^?([\\d\\.]+);', pragma_line)\n",
    "\n",
    "    if match:\n",
    "        version = match.group(1)\n",
    "        return version\n",
    "    return None\n",
    "\n",
    "def extract_non_vulnerable_lines(vulnerable_lines, num_samples):\n",
    "    non_vulnerable_lines = []\n",
    "    \n",
    "    # Load all contract files\n",
    "    contract_files = []\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.sol'):\n",
    "                contract_files.append(os.path.join(root, file))\n",
    "    \n",
    "    # Extract non-vulnerable lines\n",
    "    while len(non_vulnerable_lines) < num_samples:\n",
    "        # Randomly select a contract file\n",
    "        contract_file = random.choice(contract_files)\n",
    "        \n",
    "        # Read the file content\n",
    "        try:\n",
    "            with open(contract_file, 'r', encoding='utf-8') as file:\n",
    "                file_lines = file.readlines()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {contract_file}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Randomly select a line\n",
    "        line_number = random.randint(0, len(file_lines) - 1)\n",
    "        code_line = file_lines[line_number].strip()\n",
    "        pragma= get_pragma_version(contract_file)\n",
    "        \n",
    "        # Check if line is a comment or blank\n",
    "        if re.match(r'^\\s*(//|$)', code_line):\n",
    "            continue\n",
    "        \n",
    "        # Check if the exact line from the contract is already in vulnerable lines\n",
    "        \n",
    "        if any((line['code'] == code_line and line[\"full_path\"]==contract_file and line[\"line_number\"]==line_number) for line in vulnerable_lines_data):\n",
    "            print(\"duplicate\")\n",
    "            continue\n",
    "        \n",
    "        # Add to non-vulnerable lines\n",
    "        non_vulnerable_lines.append({\n",
    "            'contract_path': os.path.relpath(contract_file, base_path),\n",
    "            'full_path': contract_file,\n",
    "            'line_number': line_number + 1,\n",
    "            'code': code_line,\n",
    "            'pragma_version':pragma\n",
    "        })\n",
    "    \n",
    "    return non_vulnerable_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n",
      "[{'contract_path': 'dataset/arithmetic/overflow_single_tx.sol', 'full_path': '../dataset/smartbugs-curated/dataset/arithmetic/overflow_single_tx.sol', 'line_number': 2, 'code': '* @source: https://github.com/ConsenSys/evm-analyzer-benchmark-suite', 'pragma_version': '0.4.23'}, {'contract_path': 'dataset/reentrancy/0x4320e6f8c05b27ab4707cd1f6d5ce6f3e4b3a5a1.sol', 'full_path': '../dataset/smartbugs-curated/dataset/reentrancy/0x4320e6f8c05b27ab4707cd1f6d5ce6f3e4b3a5a1.sol', 'line_number': 32, 'code': '}', 'pragma_version': '0.4.19'}, {'contract_path': 'dataset/reentrancy/etherstore.sol', 'full_path': '../dataset/smartbugs-curated/dataset/reentrancy/etherstore.sol', 'line_number': 5, 'code': '*/', 'pragma_version': '0.4.10'}]\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "non_vulnerable_lines= extract_non_vulnerable_lines(vulnerable_lines, len(vulnerable_lines_data))\n",
    "print(len(non_vulnerable_lines))\n",
    "print(non_vulnerable_lines[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## storage data\n",
    "final_data= vulnerable_lines_data + non_vulnerable_lines\n",
    "with open(f'../dataset/{dataset}_final_data.json', 'w') as file:\n",
    "    json.dump(final_data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Find a way to represent the data\n",
    "Feature Engineering Pipeline\n",
    "#### 1. Tokenization (code)\n",
    "\n",
    "    Tokenize Solidity code by splitting on non-alphanumeric characters.\n",
    "    Remove comments and unnecessary whitespace.\n",
    "    Convert tokens into numerical representations (e.g., TF-IDF, one-hot encoding, or embeddings).\n",
    "\n",
    "#### 2. Presence of External Calls\n",
    "\n",
    "    Check if the line contains low-level function calls like:\n",
    "        call, delegatecall, staticcall, send, transfer\n",
    "    Store this as a binary feature (1 if present, 0 otherwise).\n",
    "\n",
    "#### 3. Use of require or assert\n",
    "\n",
    "    Check if require(...) or assert(...) appears in the line.\n",
    "    Store as a binary feature (1 if present, 0 otherwise).\n",
    "\n",
    "#### 4. Encoding Categorical Features\n",
    "\n",
    "    pragma_version: Convert Solidity versions into numerical features (e.g., split into major, minor, and patch).\n",
    "    vulnerability_category: Use label encoding or one-hot encoding.\n",
    "    contract_name: Encode as a categorical variable (or use hashing to avoid high-dimensionality).\n",
    "\n",
    "#### 5. Normalization of line_number\n",
    "\n",
    "    Scale it to a [0,1] range using Min-Max Scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "# Helper function to tokenize Solidity code\n",
    "def tokenize_code(code):\n",
    "    # Remove comments\n",
    "    code = re.sub(r\"//.*|/\\*[\\s\\S]*?\\*/\", \"\", code)\n",
    "    # Tokenize by splitting on non-alphanumeric characters\n",
    "    tokens = re.findall(r\"\\w+\", code)\n",
    "    return tokens\n",
    "\n",
    "# Feature extraction function\n",
    "def extract_features(data):\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    #Code Encoding\n",
    "    code_enc = LabelEncoder()\n",
    "    df[\"code_enc\"] = code_enc.fit_transform(df[\"code\"])\n",
    "    # Tokenization\n",
    "    df[\"tokens\"] = df[\"code\"].apply(tokenize_code)\n",
    "    df['tokens']= df['tokens'].apply(len)\n",
    "    \n",
    "    # Presence of external calls\n",
    "    external_calls = [\"call\", \"delegatecall\", \"staticcall\", \"send\", \"transfer\"]\n",
    "    df[\"has_external_call\"] = df[\"code\"].apply(lambda x: any(call in x for call in external_calls)).astype(int)\n",
    "    \n",
    "    # Presence of `require` or `assert`\n",
    "    df[\"has_require_assert\"] = df[\"code\"].apply(lambda x: \"require\" in x or \"assert\" in x).astype(int)\n",
    "\n",
    "    # Encoding categorical variables\n",
    "    df[\"pragma_version\"] = df[\"pragma_version\"].apply(lambda x: tuple(map(int, x.lstrip(\"^\").split(\".\"))) if x else (0, 0, 0))\n",
    "    df[[\"pragma_major\", \"pragma_minor\", \"pragma_patch\"]] = pd.DataFrame(df[\"pragma_version\"].tolist(), index=df.index)\n",
    "\n",
    "    label_enc = LabelEncoder()\n",
    "    #df[\"vulnerability_category\"] = label_enc.fit_transform(df[\"vulnerability_category\"])\n",
    "\n",
    "    #df[\"contract_name\"] = label_enc.fit_transform(df[\"contract_name\"])\n",
    "\n",
    "    # Normalize `line_number`\n",
    "    scaler = MinMaxScaler()\n",
    "    df[\"line_number\"] = scaler.fit_transform(df[[\"line_number\"]])\n",
    "\n",
    "    # vulnerable if vulnerability category is not empty\n",
    "    df[\"label\"]= df[\"vulnerability_category\"].apply(lambda x: False if pd.isna(x) else True)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "\n",
    "    return df.drop(columns=[\"pragma_version\", \"code\", \"contract_path\", \"full_path\", \"source\", \"vulnerability_category\",\"contract_name\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_number</th>\n",
       "      <th>code_enc</th>\n",
       "      <th>tokens</th>\n",
       "      <th>has_external_call</th>\n",
       "      <th>has_require_assert</th>\n",
       "      <th>pragma_major</th>\n",
       "      <th>pragma_minor</th>\n",
       "      <th>pragma_patch</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012165</td>\n",
       "      <td>218</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015004</td>\n",
       "      <td>219</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010543</td>\n",
       "      <td>209</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007705</td>\n",
       "      <td>112</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006894</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0.070154</td>\n",
       "      <td>280</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.007705</td>\n",
       "      <td>278</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0.010543</td>\n",
       "      <td>181</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.005272</td>\n",
       "      <td>177</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.006488</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>222 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     line_number  code_enc  tokens  has_external_call  has_require_assert  \\\n",
       "0       0.012165       218       5                  1                   1   \n",
       "1       0.015004       219       5                  1                   1   \n",
       "2       0.010543       209       4                  0                   1   \n",
       "3       0.007705       112       2                  0                   0   \n",
       "4       0.006894       132       2                  0                   0   \n",
       "..           ...       ...     ...                ...                 ...   \n",
       "217     0.070154       280       3                  1                   0   \n",
       "218     0.007705       278       3                  1                   0   \n",
       "219     0.010543       181       5                  1                   0   \n",
       "220     0.005272       177       4                  1                   0   \n",
       "221     0.006488        73       2                  1                   0   \n",
       "\n",
       "     pragma_major  pragma_minor  pragma_patch  label  \n",
       "0               0             4            22   True  \n",
       "1               0             4            22   True  \n",
       "2               0             4            25   True  \n",
       "3               0             4            24   True  \n",
       "4               0             4            24   True  \n",
       "..            ...           ...           ...    ...  \n",
       "217             0             4             0   True  \n",
       "218             0             4            18   True  \n",
       "219             0             4            18   True  \n",
       "220             0             4             0   True  \n",
       "221             0             4            25   True  \n",
       "\n",
       "[222 rows x 9 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data = extract_features(final_data)\n",
    "encoded_data[encoded_data['label']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data.to_csv(f'../dataset/{dataset}_encoded_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 355 samples\n",
      "Test data: 89 samples\n",
      "Train data: label\n",
      "True     178\n",
      "False    177\n",
      "Name: count, dtype: int64\n",
      "Test data: label\n",
      "False    45\n",
      "True     44\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Now random split for training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(encoded_data, test_size=0.2, random_state=42, stratify=encoded_data[\"label\"])\n",
    "train_data.to_csv(f'../dataset/{dataset}_train_data.csv', index=False)\n",
    "test_data.to_csv(f'../dataset/{dataset}_test_data.csv', index=False)\n",
    "\n",
    "print(f\"Train data: {len(train_data)} samples\")\n",
    "print(f\"Test data: {len(test_data)} samples\")\n",
    "print(f\"Train data: {train_data['label'].value_counts()}\")\n",
    "print(f\"Test data: {test_data['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
